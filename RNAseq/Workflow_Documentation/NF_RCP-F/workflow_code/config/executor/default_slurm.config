/* Configuration for running using on an HPC with a slurm scheduler
*/
process {
  executor='slurm' // run using slurm backend (unless a process specifies otherwise), Nextflow will generate the sbatch script and submit for you
  memory = 2.GB // Default for all tasks that do not designate something specific
  cache = 'lenient' // Disables timestamp check in caching, default caching includes timestamp and this can be broken on systems with file partitions that vary timestamping in unexpected ways

  withLabel: networkBound {
    executor = 'local' // example of a label based override, this means the local executor is used for download heavy processes.  Local in this context means the resources allocated under the Nextflow manager sbatch script.
    maxForks = 1 // maxForks is number of parallel tasks allowed.  For downloads, downloading one thing at a time seems to make sense.
  }

  withLabel: maxCPU {
    cpus = 8
  }

  withLabel: big_mem {
    memory = 32.GB
  }

  withLabel: align_mem {
    memory = 64.GB
  }

  withName: "BUILD_STAR|BUILD_RSEM" {
    cpus = 16
    memory = 60.GB
  }

  withName: "COUNT_ALIGNED|RSEQC_ALL" {
    cpus = 8
  }

  withName: DGE_BY_DESEQ2 {
    cpus = 2
    memory = 8.GB
  }
}

executor { // entire executor scope parameters. I.E. parameters that define how an entire executor should operate
  // Again this refers to the resources allocated in the initial user-submitted sbatch script
  $local {
      cpus = 2 // should match the inital user-submitted sbatch script, MAY be entirely redundant, need to test further
  }

  // this refers to jobs launched via the slurm executor (I typically have the local executor handle downloads and very fast jobs like concatting ERCC fasta to ref fasta while the slurm executor handles longer running and more resource-intensive tasks)
  $slurm {
      queueSize = 32
  }
}
