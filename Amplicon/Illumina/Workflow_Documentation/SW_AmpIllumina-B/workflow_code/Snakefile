############################################################################################
## Snakefile for GeneLab's Illumina amplicon workflow                                     ##
## Version 1.2.1                                                                          ##
## Initially developed by Michael D. Lee (Mike.Lee@nasa.gov)                              ##
## Developed and maintained by Michael D. Lee and Alexis Torres (alexis.torres@nasa.gov)  ##
############################################################################################

import os

configfile: "config.yaml"

enable_visualizations = config["enable_visualizations"]

########################################
############# General Info #############
########################################

"""
See the corresponding 'config.yaml' file for general use information.
Variables that may need to be adjusted should be changed there, not here.
"""

## example usage command ##
# snakemake --use-conda --conda-prefix ${CONDA_PREFIX}/envs -j 2 -p

# `--use-conda` – this specifies to use the conda environments included in the workflow
# `--conda-prefix` – this allows us to point to where the needed conda environments should be stored. Including this means if we use the workflow on a different dataset somewhere else in the future, it will re-use the same conda environments rather than make new ones. The value listed here, `${CONDA_PREFIX}/envs`, is the default location for conda environments (the variable `${CONDA_PREFIX}` will be expanded to the appropriate location on whichever system it is run on).
# `-j` – this lets us set how many jobs Snakemake should run concurrently (keep in mind that many of the thread and cpu parameters set in the config.yaml file will be multiplied by this)
# `-p` – specifies to print out each command being run to the screen

# See `snakemake -h` for more options and details.

########################################
####### Assay-specific GL suffix #######
########################################

assay_suffix = "GLAmpSeq"


########################################
#### Reading samples file into list ####
########################################

sample_IDs_file = config["sample_info_file"]
sample_ID_list = [line.strip() for line in open(sample_IDs_file)]

# making sure there are all unique names
if len(set(sample_ID_list)) != len(sample_ID_list):

    print("\n    Not all sample IDs in the " + str(config["sample_info_file"]) + " file are unique :(\n")
    print("    Exiting for now.\n")
    exit()

########################################
######## Setting up directories ########
########################################

# Initialize the list of needed directories without plots_dir
if config["trim_primers"] == "TRUE":
    needed_dirs = [
        config["info_out_dir"],
        config["fastqc_out_dir"],
        config["trimmed_reads_dir"],
        config["filtered_reads_dir"],
        config["final_outputs_dir"],
        "benchmarks"
    ]
else:
    needed_dirs = [
        config["info_out_dir"],
        config["fastqc_out_dir"],
        config["filtered_reads_dir"],
        config["final_outputs_dir"],
        "benchmarks"
    ]

# Conditionally add plots_dir if enable_visualizations is True
if enable_visualizations  == "TRUE":
    needed_dirs.append(config["plots_dir"])

# Try to create the directories
for dir in needed_dirs:
    try:
        os.makedirs(dir, exist_ok=True)
    except Exception as e:
        print(f"Could not create directory {dir}: {e}")

########################################
########## Setting up outputs ##########
########################################
        
# Base rule all inputs (final outs) for PE, with or without trimming
base_PE_inputs = [
    expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R1_suffix"], ID = sample_ID_list),
    expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R2_suffix"], ID = sample_ID_list),
    config["final_outputs_dir"] + config["output_prefix"] + f"taxonomy_{assay_suffix}.tsv",
    config["final_outputs_dir"] + config["output_prefix"] + f"taxonomy-and-counts_{assay_suffix}.biom.zip",
    config["final_outputs_dir"] + config["output_prefix"] + f"ASVs_{assay_suffix}.fasta",
    config["final_outputs_dir"] + config["output_prefix"] + f"read-count-tracking_{assay_suffix}.tsv",
    config["final_outputs_dir"] + config["output_prefix"] + f"counts_{assay_suffix}.tsv",
    config["final_outputs_dir"] + config["output_prefix"] + f"taxonomy-and-counts_{assay_suffix}.tsv",
    config["fastqc_out_dir"] + config["output_prefix"] + f"raw_multiqc_{assay_suffix}_report.zip",
    config["fastqc_out_dir"] + config["output_prefix"] + f"filtered_multiqc_{assay_suffix}_report.zip"
]

# Base rule all inputs (final outs) for SE, with or without trimming
base_SE_inputs = [
    expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R1_suffix"], ID = sample_ID_list),
    config["final_outputs_dir"] + config["output_prefix"] + f"taxonomy_{assay_suffix}.tsv",
    config["final_outputs_dir"] + config["output_prefix"] + f"taxonomy-and-counts_{assay_suffix}.biom.zip",
    config["final_outputs_dir"] + config["output_prefix"] + f"ASVs_{assay_suffix}.fasta",
    config["final_outputs_dir"] + config["output_prefix"] + f"read-count-tracking_{assay_suffix}.tsv",
    config["final_outputs_dir"] + config["output_prefix"] + f"counts_{assay_suffix}.tsv",
    config["final_outputs_dir"] + config["output_prefix"] + f"taxonomy-and-counts_{assay_suffix}.tsv",
    config["fastqc_out_dir"] + config["output_prefix"] + f"raw_multiqc_{assay_suffix}_report.zip",
    config["fastqc_out_dir"] + config["output_prefix"] + f"filtered_multiqc_{assay_suffix}_report.zip"
]

# Add additional inputs for trimming
if config["trim_primers"] == "TRUE":
    if config["data_type"] == "PE":
        base_PE_inputs += [
            expand(config["trimmed_reads_dir"] + "{ID}" + config["primer_trimmed_R1_suffix"], ID = sample_ID_list),
            expand(config["trimmed_reads_dir"] + "{ID}" + config["primer_trimmed_R2_suffix"], ID = sample_ID_list),
            config["trimmed_reads_dir"] + config["output_prefix"] + f"cutadapt_{assay_suffix}.log",
            config["trimmed_reads_dir"] + config["output_prefix"] + f"trimmed-read-counts_{assay_suffix}.tsv",
        ]
    else: # SE with primer trimming
        base_SE_inputs += [
            expand(config["trimmed_reads_dir"] + "{ID}" + config["primer_trimmed_R1_suffix"], ID = sample_ID_list),
            config["trimmed_reads_dir"] + config["output_prefix"] + f"cutadapt_{assay_suffix}.log",
            config["trimmed_reads_dir"] + config["output_prefix"] + f"trimmed-read-counts_{assay_suffix}.tsv",
        ]

# Conditional addition of visualization outputs (color legend only to keep it simple)
visualization_outputs = [config["plots_dir"] + config["output_prefix"] + f"color_legend_{assay_suffix}.png"] if enable_visualizations == "TRUE" else []

########################################
############# Rules start ##############
########################################

#### rules if paired-end data ####
if config["data_type"] == "PE":

    rule all:
        input: base_PE_inputs + visualization_outputs
        shell:
            """
            bash scripts/combine-benchmarks.sh
            python scripts/copy_info.py 
            """


    # R processing rule for paired-end data
    if config["trim_primers"] == "TRUE":

        rule run_R_PE:
            conda:
                "envs/R.yaml"
            input:
                expand(config["trimmed_reads_dir"] + "{ID}" + config["primer_trimmed_R1_suffix"], ID = sample_ID_list),
                expand(config["trimmed_reads_dir"] + "{ID}" + config["primer_trimmed_R2_suffix"], ID = sample_ID_list),
                config["trimmed_reads_dir"] + config["output_prefix"] + f"trimmed-read-counts_{assay_suffix}.tsv"
            output:
                expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R1_suffix"], ID = sample_ID_list),
                expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R2_suffix"], ID = sample_ID_list),
                config["final_outputs_dir"] + config["output_prefix"] + f"taxonomy_{assay_suffix}.tsv",
                config["final_outputs_dir"] + config["output_prefix"] + f"taxonomy-and-counts_{assay_suffix}.biom",
                config["final_outputs_dir"] + config["output_prefix"] + f"ASVs_{assay_suffix}.fasta",
                config["final_outputs_dir"] + config["output_prefix"] + f"read-count-tracking_{assay_suffix}.tsv",
                config["final_outputs_dir"] + config["output_prefix"] + f"counts_{assay_suffix}.tsv",
                config["final_outputs_dir"] + config["output_prefix"] + f"taxonomy-and-counts_{assay_suffix}.tsv"
            params:
                left_trunc = config["left_trunc"],
                right_trunc = config["right_trunc"],
                left_maxEE = config["left_maxEE"],
                right_maxEE = config["right_maxEE"],
                trim_primers = config["trim_primers"],
                trimmed_reads_dir = config["trimmed_reads_dir"],
                filtered_reads_dir = config["filtered_reads_dir"],
                primer_trimmed_R1_suffix = config["primer_trimmed_R1_suffix"],
                primer_trimmed_R2_suffix = config["primer_trimmed_R2_suffix"],
                filtered_R1_suffix = config["filtered_R1_suffix"],
                filtered_R2_suffix = config["filtered_R2_suffix"],
                final_outputs_dir = config["final_outputs_dir"],
                target_region = config["target_region"],
                output_prefix = config["output_prefix"],
                concatenate_reads_only = config["concatenate_reads_only"],
                assay_suffix = assay_suffix
            resources:
                mem_mb = 200000,
                cpus = 10
            log:
                "R-processing.log"
            benchmark:
                "benchmarks/run_R-benchmarks.tsv"
            shell:
                """
                Rscript scripts/Illumina-PE-R-processing.R "{params.left_trunc}" "{params.right_trunc}" "{params.left_maxEE}" "{params.right_maxEE}" "{params.trim_primers}" "{sample_IDs_file}" "{params.trimmed_reads_dir}" "{params.filtered_reads_dir}" "{params.primer_trimmed_R1_suffix}" "{params.primer_trimmed_R2_suffix}" "{params.filtered_R1_suffix}" "{params.filtered_R2_suffix}" "{params.final_outputs_dir}" "{params.output_prefix}" "{params.target_region}" "{params.concatenate_reads_only}" "{params.assay_suffix}" > {log} 2>&1
                """

    # if we did not trim the primers
    else:

        rule run_R_PE:
            conda:
                "envs/R.yaml"
            input:
                expand(config["raw_reads_dir"] + "{ID}" + config["raw_R1_suffix"], ID = sample_ID_list),
                expand(config["raw_reads_dir"] + "{ID}" + config["raw_R2_suffix"], ID = sample_ID_list)
            output:
                expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R1_suffix"], ID = sample_ID_list),
                expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R2_suffix"], ID = sample_ID_list),
                config["final_outputs_dir"] + config["output_prefix"] + f"taxonomy_{assay_suffix}.tsv",
                config["final_outputs_dir"] + config["output_prefix"] + f"taxonomy-and-counts_{assay_suffix}.biom",
                config["final_outputs_dir"] + config["output_prefix"] + f"ASVs_{assay_suffix}.fasta",
                config["final_outputs_dir"] + config["output_prefix"] + f"read-count-tracking_{assay_suffix}.tsv",
                config["final_outputs_dir"] + config["output_prefix"] + f"counts_{assay_suffix}.tsv",
                config["final_outputs_dir"] + config["output_prefix"] + f"taxonomy-and-counts_{assay_suffix}.tsv"
            params:
                left_trunc = config["left_trunc"],
                right_trunc = config["right_trunc"],
                left_maxEE = config["left_maxEE"],
                right_maxEE = config["right_maxEE"],
                trim_primers = config["trim_primers"],
                raw_reads_dir = config["raw_reads_dir"],
                filtered_reads_dir = config["filtered_reads_dir"],
                raw_R1_suffix = config["raw_R1_suffix"],
                raw_R2_suffix = config["raw_R2_suffix"],
                filtered_R1_suffix = config["filtered_R1_suffix"],
                filtered_R2_suffix = config["filtered_R2_suffix"],
                final_outputs_dir = config["final_outputs_dir"],
                target_region = config["target_region"],
                output_prefix = config["output_prefix"],
                concatenate_reads_only = config["concatenate_reads_only"],
                assay_suffix = assay_suffix
            resources:
                mem_mb = 200000,
                cpus = 10
            log:
                "R-processing.log"
            benchmark:
                "benchmarks/run_R-benchmarks.tsv"
            shell:
                """
                Rscript scripts/Illumina-PE-R-processing.R "{params.left_trunc}" "{params.right_trunc}" "{params.left_maxEE}" "{params.right_maxEE}" "{params.trim_primers}" "{sample_IDs_file}" "{params.raw_reads_dir}" "{params.filtered_reads_dir}" "{params.raw_R1_suffix}" "{params.raw_R2_suffix}" "{params.filtered_R1_suffix}" "{params.filtered_R2_suffix}" "{params.final_outputs_dir}" "{params.output_prefix}" "{params.target_region}" "{params.concatenate_reads_only}" "{params.assay_suffix}" > {log} 2>&1
                """


    # cutadapt rule for paired-end data
    rule cutadapt_PE:
        """ this rule runs cutadapt. It is only executed if config["trim_primers"] is "TRUE" """
        conda:
            "envs/cutadapt.yaml"
        input:
            R1 = config["raw_reads_dir"] + "{ID}" + config["raw_R1_suffix"],
            R2 = config["raw_reads_dir"] + "{ID}" + config["raw_R2_suffix"]
        output:
            R1 = config["trimmed_reads_dir"] + "{ID}" + config["primer_trimmed_R1_suffix"],
            R2 = config["trimmed_reads_dir"] + "{ID}" + config["primer_trimmed_R2_suffix"],
            log = config["trimmed_reads_dir"] + "{ID}-cutadapt.log",
            trim_counts = config["trimmed_reads_dir"] + "{ID}-trimmed-counts.tsv"
        params:
            F_linked_primer = config["F_linked_primer"],
            R_linked_primer = config["R_linked_primer"],
            F_primer = config["F_primer"],
            R_primer = config["R_primer"],
            min_cutadapt_len = config["min_cutadapt_len"],
            primers_linked = config["primers_linked"],
            discard_untrimmed = config["discard_untrimmed"]
        log:
            config["trimmed_reads_dir"] + "{ID}-cutadapt.log"
        benchmark:
            "benchmarks/cutadapt-{ID}-benchmarks.tsv"
        shell:
            """
            # command depends on if primers are linked or not
            if [ {params.primers_linked} == "TRUE" ]; then

                if [ {params.discard_untrimmed} == "TRUE" ]; then
                    cutadapt -a {params.F_linked_primer} -A {params.R_linked_primer} -o {output.R1} -p {output.R2} --discard-untrimmed -m {params.min_cutadapt_len} {input.R1} {input.R2} > {log} 2>&1
                else
                    cutadapt -a {params.F_linked_primer} -A {params.R_linked_primer} -o {output.R1} -p {output.R2} -m {params.min_cutadapt_len} {input.R1} {input.R2} > {log} 2>&1
                fi

            else

                if [ {params.discard_untrimmed} == "TRUE" ]; then
                    cutadapt -g {params.F_primer} -G {params.R_primer} -o {output.R1} -p {output.R2} --discard-untrimmed -m {params.min_cutadapt_len} {input.R1} {input.R2} > {log} 2>&1
                else
                    cutadapt -g {params.F_primer} -G {params.R_primer} -o {output.R1} -p {output.R2} -m {params.min_cutadapt_len} {input.R1} {input.R2} > {log} 2>&1
                fi

            fi

            paste <( printf "{wildcards.ID}" ) <( grep "read pairs processed" {output.log} | tr -s " " "\t" | cut -f 5 | tr -d "," ) <( grep "Pairs written" {output.log} | tr -s " " "\t" | cut -f 5 | tr -d "," ) > {output.trim_counts}
            """

    # rule for raw fastqc for paired-end data
    rule raw_fastqc_PE:
        """
        This rule runs fastqc on all raw input fastq files.
        """

        conda:
            "envs/qc.yaml"
        input:
            config["raw_reads_dir"] + "{ID}" + config["raw_R1_suffix"],
            config["raw_reads_dir"] + "{ID}" + config["raw_R2_suffix"]
        output:
            config["raw_reads_dir"] + "{ID}" + config["raw_R1_suffix"].rsplit(".", 2)[0] + "_fastqc.zip",
            config["raw_reads_dir"] + "{ID}" + config["raw_R2_suffix"].rsplit(".", 2)[0] + "_fastqc.zip"
        benchmark:
            "benchmarks/raw_fastqc-{ID}-benchmarks.tsv"
        shell:
            """
            fastqc {input} -t 2 -q
            """

    # rule for raw multiqc for paired-end data
    rule raw_multiqc_PE:
        """
        This rule collates all raw fastqc outputs.
        """

        conda:
            "envs/qc.yaml"
        input:
            expand(config["raw_reads_dir"] + "{ID}" + config["raw_R1_suffix"].rsplit(".", 2)[0] + "_fastqc.zip", ID = sample_ID_list),
            expand(config["raw_reads_dir"] + "{ID}" + config["raw_R2_suffix"].rsplit(".", 2)[0] + "_fastqc.zip", ID = sample_ID_list)
        params:
            int_out_dir = config["output_prefix"] + "raw_multiqc_report",
            out_filename_prefix = config["output_prefix"] + "raw_multiqc",
            int_out_data_dir = config["output_prefix"] + "raw_multiqc_data",
            int_html_file = config["output_prefix"] + "raw_multiqc.html",
            int_zip = config["output_prefix"] + "raw_multiqc_report.zip",
            r1_html_files = expand(config["raw_reads_dir"] + "{ID}" + config["raw_R1_suffix"].rsplit(".", 2)[0] + "_fastqc.html", ID = sample_ID_list),
            r2_html_files = expand(config["raw_reads_dir"] + "{ID}" + config["raw_R2_suffix"].rsplit(".", 2)[0] + "_fastqc.html", ID = sample_ID_list),
            config_file = "config/multiqc.config"
        output:
            final_out_zip = config["fastqc_out_dir"] + config["output_prefix"] + f"raw_multiqc_{assay_suffix}_report.zip"
        benchmark:
            "benchmarks/raw_multiqc-benchmarks.tsv"
        shell:
            """
            multiqc -q -n {params.out_filename_prefix} --force --cl-config 'max_table_rows: 99999999' --interactive --config {params.config_file} {input} > /dev/null 2>&1
            
            # removing the individual fastqc files
            rm -rf {input} {params.r1_html_files} {params.r2_html_files}

            # making an output report directory and moving things into it
            mkdir -p {params.int_out_dir}
            mv {params.int_html_file} {params.int_out_data_dir} {params.int_out_dir}
            
            # zipping and removing unzipped dir
            zip -q -r {params.int_zip} {params.int_out_dir} && rm -rf {params.int_out_dir}

            # moving to final wanted location
            mv {params.int_zip} {output.final_out_zip}
            """


    # rule for filtered fastqc for paired-end data (inherits from rule raw_fastqc_PE)
    use rule raw_fastqc_PE as filtered_fastqc_PE with:
        input:
            config["filtered_reads_dir"] + "{ID}" + config["filtered_R1_suffix"],
            config["filtered_reads_dir"] + "{ID}" + config["filtered_R2_suffix"]
        output:
            config["filtered_reads_dir"] + "{ID}" + config["filtered_R1_suffix"].rsplit(".", 2)[0] + "_fastqc.zip",
            config["filtered_reads_dir"] + "{ID}" + config["filtered_R2_suffix"].rsplit(".", 2)[0] + "_fastqc.zip"
        benchmark:
            "benchmarks/filtered_fastqc-{ID}-benchmarks.tsv"


    # rule for filtered multiqc for paired-end data (inherits from raw_multiqc_PE)
    use rule raw_multiqc_PE as filtered_multiqc_PE with:
        input:
            expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R1_suffix"].rsplit(".", 2)[0] + "_fastqc.zip", ID = sample_ID_list),
            expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R2_suffix"].rsplit(".", 2)[0] + "_fastqc.zip", ID = sample_ID_list)
        params:
            int_out_dir = config["output_prefix"] + "filtered_multiqc_report",
            out_filename_prefix = config["output_prefix"] + "filtered_multiqc",
            int_out_data_dir = config["output_prefix"] + "filtered_multiqc_data",
            int_html_file = config["output_prefix"] + "filtered_multiqc.html",
            int_zip = config["output_prefix"] + "filtered_multiqc_report.zip",
            r1_html_files = expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R1_suffix"].rsplit(".", 2)[0] + "_fastqc.html", ID = sample_ID_list),
            r2_html_files = expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R2_suffix"].rsplit(".", 2)[0] + "_fastqc.html", ID = sample_ID_list),
            config_file = "config/multiqc.config"
        output:
            final_out_zip = config["fastqc_out_dir"] + config["output_prefix"] + f"filtered_multiqc_{assay_suffix}_report.zip"
        benchmark:
            "benchmarks/filtered_multiqc-benchmarks.tsv"



#### end of rules specific for paired-end data ####

##################################
#### rules if single-end data ####
##################################
if config["data_type"] == "SE":

    rule all:
        input: base_SE_inputs + visualization_outputs
        shell:
            """
            bash scripts/combine-benchmarks.sh
            python scripts/copy_info.py 
            """



    # R processing rule for single-end data
    if config["trim_primers"] == "TRUE":

        rule run_R_SE:
            conda:
                "envs/R.yaml"
            input:
                expand(config["trimmed_reads_dir"] + "{ID}" + config["primer_trimmed_R1_suffix"], ID = sample_ID_list),
                config["trimmed_reads_dir"] + config["output_prefix"] + f"trimmed-read-counts_{assay_suffix}.tsv"
            output:
                expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R1_suffix"], ID = sample_ID_list),
                config["final_outputs_dir"] + config["output_prefix"] + f"taxonomy_{assay_suffix}.tsv",
                config["final_outputs_dir"] + config["output_prefix"] + f"taxonomy-and-counts_{assay_suffix}.biom",
                config["final_outputs_dir"] + config["output_prefix"] + f"ASVs_{assay_suffix}.fasta",
                config["final_outputs_dir"] + config["output_prefix"] + f"read-count-tracking_{assay_suffix}.tsv",
                config["final_outputs_dir"] + config["output_prefix"] + f"counts_{assay_suffix}.tsv",
                config["final_outputs_dir"] + config["output_prefix"] + f"taxonomy-and-counts_{assay_suffix}.tsv"
            params:
                left_trunc = config["left_trunc"],
                left_maxEE = config["left_maxEE"],
                trim_primers = config["trim_primers"],
                trimmed_reads_dir = config["trimmed_reads_dir"],
                filtered_reads_dir = config["filtered_reads_dir"],
                primer_trimmed_R1_suffix = config["primer_trimmed_R1_suffix"],
                filtered_R1_suffix = config["filtered_R1_suffix"],
                final_outputs_dir = config["final_outputs_dir"],
                target_region = config["target_region"],
                output_prefix = config["output_prefix"],
                assay_suffix = assay_suffix
            resources:
                mem_mb = 200000,
                cpus = 10
            log:
                "R-processing.log"
            benchmark:
                "benchmarks/run_R-benchmarks.tsv"
            shell:
                """
                Rscript scripts/Illumina-SE-R-processing.R "{params.left_trunc}" "{params.left_maxEE}" "{params.trim_primers}" "{sample_IDs_file}" "{params.trimmed_reads_dir}" "{params.filtered_reads_dir}" "{params.primer_trimmed_R1_suffix}" "{params.filtered_R1_suffix}" "{params.final_outputs_dir}" "{params.output_prefix}" "{params.target_region}" "{params.assay_suffix}" > {log} 2>&1
                """

    # if we did not trim the primers
    else:

        rule run_R_SE:
            conda:
                "envs/R.yaml"
            input:
                expand(config["raw_reads_dir"] + "{ID}" + config["raw_R1_suffix"], ID = sample_ID_list)
            output:
                expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R1_suffix"], ID = sample_ID_list),
                config["final_outputs_dir"] + config["output_prefix"] + f"taxonomy_{assay_suffix}.tsv",
                config["final_outputs_dir"] + config["output_prefix"] + f"taxonomy-and-counts_{assay_suffix}.biom",
                config["final_outputs_dir"] + config["output_prefix"] + f"ASVs_{assay_suffix}.fasta",
                config["final_outputs_dir"] + config["output_prefix"] + f"read-count-tracking_{assay_suffix}.tsv",
                config["final_outputs_dir"] + config["output_prefix"] + f"counts_{assay_suffix}.tsv",
                config["final_outputs_dir"] + config["output_prefix"] + f"taxonomy-and-counts_{assay_suffix}.tsv"
            params:
                left_trunc = config["left_trunc"],
                left_maxEE = config["left_maxEE"],
                trim_primers = config["trim_primers"],
                raw_reads_dir = config["raw_reads_dir"],
                filtered_reads_dir = config["filtered_reads_dir"],
                raw_R1_suffix = config["raw_R1_suffix"],
                filtered_R1_suffix = config["filtered_R1_suffix"],
                final_outputs_dir = config["final_outputs_dir"],
                target_region = config["target_region"],
                output_prefix = config["output_prefix"],
                assay_suffix = assay_suffix
            resources:
                mem_mb = 200000,
                cpus = 10
            log:
                "R-processing.log"
            benchmark:
                "benchmarks/run_R-benchmarks.tsv"
            shell:
                """
                Rscript scripts/Illumina-SE-R-processing.R "{params.left_trunc}" "{params.left_maxEE}" "{params.trim_primers}" "{sample_IDs_file}" "{params.raw_reads_dir}" "{params.filtered_reads_dir}" "{params.raw_R1_suffix}" "{params.filtered_R1_suffix}" "{params.final_outputs_dir}" "{params.output_prefix}" "{params.target_region}" "{params.assay_suffix}" > {log} 2>&1
                """


    # cutadapt rule for single-end data
    rule cutadapt_SE:
        """ this rule runs cutadapt. It is only executed if config["trim_primers"] is "TRUE" """
        conda:
            "envs/cutadapt.yaml"
        input:
            R1 = config["raw_reads_dir"] + "{ID}" + config["raw_R1_suffix"]
        output:
            R1 = config["trimmed_reads_dir"] + "{ID}" + config["primer_trimmed_R1_suffix"],
            log = config["trimmed_reads_dir"] + "{ID}-cutadapt.log",
            trim_counts = config["trimmed_reads_dir"] + "{ID}-trimmed-counts.tsv"
        params:
            F_linked_primer = config["F_linked_primer"],
            F_primer = config["F_primer"],
            min_cutadapt_len = config["min_cutadapt_len"],
            primers_linked = config["primers_linked"],
            discard_untrimmed = config["discard_untrimmed"]
        log:
            config["trimmed_reads_dir"] + "{ID}-cutadapt.log"
        benchmark:
            "benchmarks/cutadapt-{ID}-benchmarks.tsv"
        shell:
            """
            # command depends on if primers are linked or not
            if [ {params.primers_linked} == "TRUE" ]; then

                if [ {params.discard_untrimmed} == "TRUE" ]; then
                    cutadapt -a {params.F_linked_primer} -o {output.R1} --discard-untrimmed -m {params.min_cutadapt_len} {input.R1} > {log} 2>&1
                else
                    cutadapt -a {params.F_linked_primer} -o {output.R1} -m {params.min_cutadapt_len} {input.R1} > {log} 2>&1
                fi

            else

                if [ {params.discard_untrimmed} == "TRUE" ]; then
                    cutadapt -g {params.F_primer} -o {output.R1} --discard-untrimmed -m {params.min_cutadapt_len} {input.R1} > {log} 2>&1
                else
                    cutadapt -g {params.F_primer} -o {output.R1} -m {params.min_cutadapt_len} {input.R1} > {log} 2>&1
                fi

            fi

            paste <( printf "{wildcards.ID}" ) <( grep "reads processed" {output.log} | tr -s " " "\t" | cut -f 4 | tr -d "," ) <( grep "Reads written" {output.log} | tr -s " " "\t" | cut -f 5 | tr -d "," ) > {output.trim_counts}
            """


    # rule for raw fastqc for single-end data
    rule raw_fastqc_SE:
        """
        This rule runs fastqc on all raw input fastq files.
        """

        conda:
            "envs/qc.yaml"
        input:
            config["raw_reads_dir"] + "{ID}" + config["raw_R1_suffix"]
        output:
            config["raw_reads_dir"] + "{ID}" + config["raw_R1_suffix"].rsplit(".", 2)[0] + "_fastqc.zip"
        benchmark:
            "benchmarks/raw_fastqc-{ID}-benchmarks.tsv"
        shell:
            """
            fastqc {input} -t 1 -q
            """


    # rule for raw multiqc for single-end data
    rule raw_multiqc_SE:
        """
        This rule collates all raw fastqc outputs.
        """

        conda:
            "envs/qc.yaml"
        input:
            expand(config["raw_reads_dir"] + "{ID}" + config["raw_R1_suffix"].rsplit(".", 2)[0] + "_fastqc.zip", ID = sample_ID_list)
        params:
            int_out_dir = config["output_prefix"] + "raw_multiqc_report",
            out_filename_prefix = config["output_prefix"] + "raw_multiqc",
            int_out_data_dir = config["output_prefix"] + "raw_multiqc_data",
            int_html_file = config["output_prefix"] + "raw_multiqc.html",
            int_zip = config["output_prefix"] + "raw_multiqc_report.zip",
            r1_html_files = expand(config["raw_reads_dir"] + "{ID}" + config["raw_R1_suffix"].rsplit(".", 2)[0] + "_fastqc.html", ID = sample_ID_list),
            config_file = "config/multiqc.config"
        output:
            final_out_zip = config["fastqc_out_dir"] + config["output_prefix"] + f"raw_multiqc_{assay_suffix}_report.zip"
        benchmark:
            "benchmarks/raw_multiqc-benchmarks.tsv"
        shell:
            """
            multiqc -q -n {params.out_filename_prefix} --force --cl-config 'max_table_rows: 99999999' --interactive --config {params.config_file} {input} > /dev/null 2>&1

            # removing the individual fastqc files
            rm -rf {input} {params.r1_html_files}

            # making an output report directory and moving things into it
            mkdir -p {params.int_out_dir}
            mv {params.int_html_file} {params.int_out_data_dir} {params.int_out_dir}

            # zipping and removing unzipped dir
            zip -q -r {params.int_zip} {params.int_out_dir} && rm -rf {params.int_out_dir}

            # moving to final wanted location
            mv {params.int_zip} {output.final_out_zip}
            """


    # rule for filtered fastqc for single-end data (inherits from rule raw_fastqc_SE)
    use rule raw_fastqc_SE as filtered_fastqc_SE with:
        input:
            config["filtered_reads_dir"] + "{ID}" + config["filtered_R1_suffix"]
        output:
            config["filtered_reads_dir"] + "{ID}" + config["filtered_R1_suffix"].rsplit(".", 2)[0] + "_fastqc.zip"
        benchmark:
            "benchmarks/filtered_fastqc-{ID}-benchmarks.tsv"


    # rule for filtered multiqc for single-end data (inherits from rule raw_multiqc_SE)
    use rule raw_multiqc_SE as filtered_multiqc_SE with:
        input:
            expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R1_suffix"].rsplit(".", 2)[0] + "_fastqc.zip", ID = sample_ID_list)
        params:
            int_out_dir = config["output_prefix"] + "filtered_multiqc_report",
            out_filename_prefix = config["output_prefix"] + "filtered_multiqc",
            int_out_data_dir = config["output_prefix"] + "filtered_multiqc_data",
            int_html_file = config["output_prefix"] + "filtered_multiqc.html",
            int_zip = config["output_prefix"] + "filtered_multiqc_report.zip",
            r1_html_files = expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R1_suffix"].rsplit(".", 2)[0] + "_fastqc.html", ID = sample_ID_list),
            config_file = "config/multiqc.config"
        output:
            final_out_zip = config["fastqc_out_dir"] + config["output_prefix"] + f"filtered_multiqc_{assay_suffix}_report.zip"
        benchmark:
            "benchmarks/filtered_multiqc-benchmarks.tsv"


#### end of rules specific for single-end data ####

##################################################################
#### rules that are the same whether paired-end or single-end ####
##################################################################
rule r_visualizations:
    """ This rule generates R visualizations using trimmed read data and grouping info from the runsheet"""
    conda:
        "envs/R_visualizations.yaml"
    input:
        runsheet = config["runsheet"],
        sample_info = config["sample_info_file"],
        counts = config["final_outputs_dir"] + config["output_prefix"] + f"counts_{assay_suffix}.tsv",
        taxonomy = config["final_outputs_dir"] + config["output_prefix"] + f"taxonomy_{assay_suffix}.tsv"
    output:
        # Use completion flag file in place of plot outputs for variable plots
        legend = config["plots_dir"] + config["output_prefix"] + f"color_legend_{assay_suffix}.png"
    params:
        assay_suffix = assay_suffix,
        plots_dir = config["plots_dir"],
        output_prefix = config["output_prefix"]
    resources:
        mem_mb = 200000,
        cpus = 10
    log:
        "R-visualizations.log"
    benchmark:
        "benchmarks/r-visualizations-benchmarks.tsv"
    shell:
        """
        Rscript visualizations/Illumina-R-visualizations.R "{input.runsheet}" "{input.sample_info}" "{input.counts}" "{input.taxonomy}" "{params.assay_suffix}" "{params.plots_dir}" "{params.output_prefix}"  > {log} 2>&1
        """


rule zip_biom:
    input:
        config["final_outputs_dir"] + config["output_prefix"] + f"taxonomy-and-counts_{assay_suffix}.biom"
    output:
        config["final_outputs_dir"] + config["output_prefix"] + f"taxonomy-and-counts_{assay_suffix}.biom.zip"
    shell:
        """
        zip -j -q {output} {input} && rm {input}
        """


rule combine_cutadapt_logs_and_summarize:
    """ this rule combines the cutadapt logs and summarizes them. It is only executed if config["trim_primers"] is "TRUE" """
    input:
        counts = expand(config["trimmed_reads_dir"] + "{ID}-trimmed-counts.tsv", ID = sample_ID_list),
        logs = expand(config["trimmed_reads_dir"] + "{ID}-cutadapt.log", ID = sample_ID_list)
    output:
        combined_log = config["trimmed_reads_dir"] + config["output_prefix"] + f"cutadapt_{assay_suffix}.log",
        combined_counts = config["trimmed_reads_dir"] + config["output_prefix"] + f"trimmed-read-counts_{assay_suffix}.tsv"
    benchmark:
        "benchmarks/combine_cutadapt_logs_and_summarize-benchmarks.tsv"
    shell:
        """
        cat {input.logs} > {output.combined_log}
        rm {input.logs}
        
        cat <( printf "sample\traw_reads\tcutadapt_trimmed\n" ) <( cat {input.counts} ) > {output.combined_counts}
        rm {input.counts}
        """


rule clean_all:
    shell:
        "rm -rf {needed_dirs}"