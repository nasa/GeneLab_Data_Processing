############################################################################################
## Snakefile for GeneLab's Illumina amplicon workflow                                     ##
## Developed by Michael D. Lee (Mike.Lee@nasa.gov)                                        ##
############################################################################################

import os

configfile: "config.yaml"


########################################
############# General Info #############
########################################


"""
See the corresponding 'config.yaml' file for general use information.
Variables that may need to be adjusted should be changed there, not here.
"""

## example usage command ##
# snakemake --use-conda --conda-prefix ${CONDA_PREFIX}/envs -j 2 -p

# `--use-conda` – this specifies to use the conda environments included in the workflow
# `--conda-prefix` – this allows us to point to where the needed conda environments should be stored. Including this means if we use the workflow on a different dataset somewhere else in the future, it will re-use the same conda environments rather than make new ones. The value listed here, `${CONDA_PREFIX}/envs`, is the default location for conda environments (the variable `${CONDA_PREFIX}` will be expanded to the appropriate location on whichever system it is run on).
# `-j` – this lets us set how many jobs Snakemake should run concurrently (keep in mind that many of the thread and cpu parameters set in the config.yaml file will be multiplied by this)
# `-p` – specifies to print out each command being run to the screen

# See `snakemake -h` for more options and details.


########################################
#### Reading samples file into list ####
########################################

sample_IDs_file = config["sample_info_file"]
sample_ID_list = [line.strip() for line in open(sample_IDs_file)]


########################################
######## Setting up directories ########
########################################

if config["trim_primers"] == "TRUE":
    needed_dirs = [config["fastqc_out_dir"], config["trimmed_reads_dir"], config["filtered_reads_dir"], config["final_outputs_dir"]]
else:
    needed_dirs = [config["fastqc_out_dir"], config["filtered_reads_dir"], config["final_outputs_dir"]]

for dir in needed_dirs:
    try:
        os.mkdir(dir)
    except:
        pass


########################################
############# Rules start ##############
########################################


if config["trim_primers"] == "TRUE":

    rule all:
        input:
            expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R1_suffix"], ID = sample_ID_list),
            expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R2_suffix"], ID = sample_ID_list),
            expand(config["trimmed_reads_dir"] + "{ID}" + config["primer_trimmed_R1_suffix"], ID = sample_ID_list),
            expand(config["trimmed_reads_dir"] + "{ID}" + config["primer_trimmed_R2_suffix"], ID = sample_ID_list),
            config["trimmed_reads_dir"] + "cutadapt.log",
            config["trimmed_reads_dir"] + "trimmed-read-counts.tsv",
            config["final_outputs_dir"] + "taxonomy.tsv",
            config["final_outputs_dir"] + "taxonomy-and-counts.biom.zip",
            config["final_outputs_dir"] + "ASVs.fasta",
            config["final_outputs_dir"] + "read-count-tracking.tsv",
            config["final_outputs_dir"] + "counts.tsv",
            config["final_outputs_dir"] + "taxonomy-and-counts.tsv",
            config["fastqc_out_dir"] + "raw_multiqc_data.zip",
            config["fastqc_out_dir"] + "filtered_multiqc_data.zip"
        shell:
            """
            # copying log file to store with processing info
            cp .snakemake/log/$(ls -t .snakemake/log/ | head -n 1) snakemake-run.log
            """

else:

    rule all:
        input:
            expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R1_suffix"], ID = sample_ID_list),
            expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R2_suffix"], ID = sample_ID_list),
            config["final_outputs_dir"] + "taxonomy.tsv",
            config["final_outputs_dir"] + "taxonomy-and-counts.biom.zip",
            config["final_outputs_dir"] + "ASVs.fasta",
            config["final_outputs_dir"] + "read-count-tracking.tsv",
            config["final_outputs_dir"] + "counts.tsv",
            config["final_outputs_dir"] + "taxonomy-and-counts.tsv",
            config["fastqc_out_dir"] + "raw_multiqc_data.zip",
            config["fastqc_out_dir"] + "filtered_multiqc_data.zip"
        shell:
            """
            # copying log file to store with processing info
            cp .snakemake/log/$(ls -t .snakemake/log/ | head -n 1) snakemake-run.log
            """


rule zip_biom:
    input:
        config["final_outputs_dir"] + "taxonomy-and-counts.biom"
    output:
        config["final_outputs_dir"] + "taxonomy-and-counts.biom.zip"
    params:
        final_outputs_dir = config["final_outputs_dir"]
    shell:
        """
        zip -q {params.final_outputs_dir}taxonomy-and-counts.biom.zip {params.final_outputs_dir}taxonomy-and-counts.biom && rm {params.final_outputs_dir}taxonomy-and-counts.biom
        """


if config["trim_primers"] == "TRUE":

    rule run_R:
        input:
            expand(config["trimmed_reads_dir"] + "{ID}" + config["primer_trimmed_R1_suffix"], ID = sample_ID_list),
            expand(config["trimmed_reads_dir"] + "{ID}" + config["primer_trimmed_R2_suffix"], ID = sample_ID_list)
        output:
            expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R1_suffix"], ID = sample_ID_list),
            expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R2_suffix"], ID = sample_ID_list),
            config["final_outputs_dir"] + "taxonomy.tsv",
            config["final_outputs_dir"] + "taxonomy-and-counts.biom",
            config["final_outputs_dir"] + "ASVs.fasta",
            config["final_outputs_dir"] + "read-count-tracking.tsv",
            config["final_outputs_dir"] + "counts.tsv",
            config["final_outputs_dir"] + "taxonomy-and-counts.tsv"
        params:
            left_trunc = config["left_trunc"],
            right_trunc = config["right_trunc"],
            left_maxEE = config["left_maxEE"],
            right_maxEE = config["right_maxEE"],
            trim_primers = config["trim_primers"],
            trimmed_reads_dir = config["trimmed_reads_dir"],
            filtered_reads_dir = config["filtered_reads_dir"],
            primer_trimmed_R1_suffix = config["primer_trimmed_R1_suffix"],
            primer_trimmed_R2_suffix = config["primer_trimmed_R2_suffix"],
            filtered_R1_suffix = config["filtered_R1_suffix"],
            filtered_R2_suffix = config["filtered_R2_suffix"],
            final_outputs_dir = config["final_outputs_dir"]
        log:
            "R-processing.log"
        shell:
            """
            Rscript scripts/full-R-processing.R "{params.left_trunc}" "{params.right_trunc}" "{params.left_maxEE}" "{params.right_maxEE}" "{params.trim_primers}" "{sample_IDs_file}" "{params.trimmed_reads_dir}" "{params.filtered_reads_dir}" "{params.primer_trimmed_R1_suffix}" "{params.primer_trimmed_R2_suffix}" "{params.filtered_R1_suffix}" "{params.filtered_R2_suffix}" "{params.final_outputs_dir}" > {log} 2>&1
            """

else:

    rule run_R:
        input:
            expand(config["raw_reads_dir"] + "{ID}" + config["raw_R1_suffix"], ID = sample_ID_list),
            expand(config["raw_reads_dir"] + "{ID}" + config["raw_R2_suffix"], ID = sample_ID_list)
        output:
            expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R1_suffix"], ID = sample_ID_list),
            expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R2_suffix"], ID = sample_ID_list),
            config["final_outputs_dir"] + "taxonomy.tsv",
            config["final_outputs_dir"] + "taxonomy-and-counts.biom",
            config["final_outputs_dir"] + "ASVs.fasta",
            config["final_outputs_dir"] + "read-count-tracking.tsv",
            config["final_outputs_dir"] + "counts.tsv",
            config["final_outputs_dir"] + "taxonomy-and-counts.tsv"
        params:
            left_trunc = config["left_trunc"],
            right_trunc = config["right_trunc"],
            left_maxEE = config["left_maxEE"],
            right_maxEE = config["right_maxEE"],
            trim_primers = config["trim_primers"],
            raw_reads_dir = config["raw_reads_dir"],
            filtered_reads_dir = config["filtered_reads_dir"],
            raw_R1_suffix = config["raw_R1_suffix"],
            raw_R2_suffix = config["raw_R2_suffix"],
            filtered_R1_suffix = config["filtered_R1_suffix"],
            filtered_R2_suffix = config["filtered_R2_suffix"],
            final_outputs_dir = config["final_outputs_dir"]
        log:
            "R-processing.log"
        shell:
            """
            Rscript scripts/full-R-processing.R "{params.left_trunc}" "{params.right_trunc}" "{params.left_maxEE}" "{params.right_maxEE}" "{params.trim_primers}" "{sample_IDs_file}" "{params.raw_reads_dir}" "{params.filtered_reads_dir}" "{params.raw_R1_suffix}" "{params.raw_R2_suffix}" "{params.filtered_R1_suffix}" "{params.filtered_R2_suffix}" "{params.final_outputs_dir}" > {log} 2>&1
            """


rule combine_cutadapt_logs_and_summarize:
    """ this rule combines the cutadapt logs and summarizes them. It is only executed if config["trim_primers"] is "TRUE" """
    input:
        counts = expand(config["trimmed_reads_dir"] + "{ID}-trimmed-counts.tsv", ID = sample_ID_list),
        logs = expand(config["trimmed_reads_dir"] + "{ID}-cutadapt.log", ID = sample_ID_list)
    output:
        combined_log = config["trimmed_reads_dir"] + "cutadapt.log",
        combined_counts = config["trimmed_reads_dir"] + "trimmed-read-counts.tsv"
    shell:
        """
        cat {input.logs} > {output.combined_log}
        rm {input.logs}
        
        cat <( printf "sample\traw_reads\tcutadapt_trimmed\n" ) <( cat {input.counts} ) > {output.combined_counts}
        rm {input.counts}
        """


rule cutadapt:
    """ this rule runs cutadapt. It is only executed if config["trim_primers"] is "TRUE" """
    conda:
        "envs/cutadapt.yaml"
    input:
        R1 = config["raw_reads_dir"] + "{ID}" + config["raw_R1_suffix"],
        R2 = config["raw_reads_dir"] + "{ID}" + config["raw_R2_suffix"]
    output:
        R1 = config["trimmed_reads_dir"] + "{ID}" + config["primer_trimmed_R1_suffix"],
        R2 = config["trimmed_reads_dir"] + "{ID}" + config["primer_trimmed_R2_suffix"],
        log = config["trimmed_reads_dir"] + "{ID}-cutadapt.log",
        trim_counts = config["trimmed_reads_dir"] + "{ID}-trimmed-counts.tsv"
    params:
        F_linked_primer = config["F_linked_primer"],
        R_linked_primer = config["R_linked_primer"],
        F_primer = config["F_primer"],
        R_primer = config["R_primer"],
        min_cutadapt_len = config["min_cutadapt_len"],
        primers_linked = config["primers_linked"]
    log:
        config["trimmed_reads_dir"] + "{ID}-cutadapt.log"
    shell:
        """
        # command depends on if primers are linked or not
        if [ {params.primers_linked} == "TRUE" ]; then
            cutadapt -a {params.F_linked_primer} -A {params.R_linked_primer} -o {output.R1} -p {output.R2} --discard-untrimmed -m {params.min_cutadapt_len} {input.R1} {input.R2} > {log} 2>&1
        else
            cutadapt -g {params.F_primer} -G {params.R_primer} -o {output.R1} -p {output.R2} --discard-untrimmed -m {params.min_cutadapt_len} {input.R1} {input.R2} > {log} 2>&1
        fi

        paste <( printf "{wildcards.ID}" ) <( grep "read pairs processed" {output.log} | tr -s " " "\t" | cut -f 5 | tr -d "," ) <( grep "Pairs written" {output.log} | tr -s " " "\t" | cut -f 5 | tr -d "," ) > {output.trim_counts}
        """


rule filtered_multiqc:
    """
    This rule collates all trimmed/filtered fastqc outputs.
    """

    conda:
        "envs/qc.yaml"
    input:
        expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R1_suffix"].rsplit(".", 2)[0] + "_fastqc.zip", ID = sample_ID_list),
        expand(config["filtered_reads_dir"] + "{ID}" + config["filtered_R2_suffix"].rsplit(".", 2)[0] + "_fastqc.zip", ID = sample_ID_list)
    params:
        fastqc_out_dir = config["fastqc_out_dir"],
        filtered_reads_dir = config["filtered_reads_dir"]
    output:
        config["fastqc_out_dir"] + "filtered_multiqc.html",
        config["fastqc_out_dir"] + "filtered_multiqc_data.zip"
    shell:
        """
        multiqc -z -q -o {params.fastqc_out_dir} -n filtered_multiqc  {params.filtered_reads_dir} > /dev/null 2>&1
        # removing the individual fastqc files and temp locations
        rm -rf {params.filtered_reads_dir}*fastqc*
        """



rule filtered_fastqc:
    """
    This rule runs fastqc on all trimmed/filtered input fastq files.
    """

    conda:
        "envs/qc.yaml"
    input:
        config["filtered_reads_dir"] + "{ID}" + config["filtered_R1_suffix"],
        config["filtered_reads_dir"] + "{ID}" + config["filtered_R2_suffix"]
    output:
        config["filtered_reads_dir"] + "{ID}" + config["filtered_R1_suffix"].rsplit(".", 2)[0] + "_fastqc.zip",
        config["filtered_reads_dir"] + "{ID}" + config["filtered_R2_suffix"].rsplit(".", 2)[0] + "_fastqc.zip"
    shell:
        """
        fastqc {input} -t 2 -q
        """


rule raw_multiqc:
    """
    This rule collates all raw fastqc outputs.
    """

    conda:
        "envs/qc.yaml"
    input:
        expand(config["raw_reads_dir"] + "{ID}" + config["raw_R1_suffix"].rsplit(".", 2)[0] + "_fastqc.zip", ID = sample_ID_list),
        expand(config["raw_reads_dir"] + "{ID}" + config["raw_R2_suffix"].rsplit(".", 2)[0] + "_fastqc.zip", ID = sample_ID_list)
    params:
        raw_reads_dir = config["raw_reads_dir"],
        fastqc_out_dir = config["fastqc_out_dir"]
    output:
        config["fastqc_out_dir"] + "raw_multiqc.html",
        config["fastqc_out_dir"] + "raw_multiqc_data.zip"
    shell:
        """
        multiqc -z -q -o {params.fastqc_out_dir} -n raw_multiqc {params.raw_reads_dir} > /dev/null 2>&1
        # removing the individual fastqc files
        rm -rf {params.raw_reads_dir}*fastqc*
        """


rule raw_fastqc:
    """
    This rule runs fastqc on all raw input fastq files.
    """

    conda:
        "envs/qc.yaml"
    input:
        config["raw_reads_dir"] + "{ID}" + config["raw_R1_suffix"],
        config["raw_reads_dir"] + "{ID}" + config["raw_R2_suffix"]
    output:
        config["raw_reads_dir"] + "{ID}" + config["raw_R1_suffix"].rsplit(".", 2)[0] + "_fastqc.zip",
        config["raw_reads_dir"] + "{ID}" + config["raw_R2_suffix"].rsplit(".", 2)[0] + "_fastqc.zip"
    shell:
        """
        fastqc {input} -t 2 -q
        """

rule clean_all:
    shell:
        "rm -rf {needed_dirs} .snakemake/"
