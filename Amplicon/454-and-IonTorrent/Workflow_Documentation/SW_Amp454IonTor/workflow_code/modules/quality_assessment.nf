#!/usr/bin/env nextflow
nextflow.enable.dsl = 2

/**************************************************************************************** 
*********************  Sequence quality assessment and control processes ****************
****************************************************************************************/

// A 2-column (single-end) or 3-column (paired-end) file
params.csv_file = "${baseDir}/file.csv" 
params.prefix = "raw"

// FastQC performed on reads
process FASTQC {

    tag "Running fastqc on ${sample_id}..."
    beforeScript "chmod +x ${baseDir}/bin/*"
    label "fastqc"

    input:
        tuple val(sample_id), path(reads)
    output:
        tuple path("*.html"), path("*.zip")
    script:
        """
        fastqc -o . \\
        -t ${task.cpus} -q \\
        ${reads}
        """
}


process MULTIQC {

    tag "Running multiqc on the ${prefix} files..."
    beforeScript "chmod +x ${baseDir}/bin/*"
    
    input:
        val(prefix)
        path(files)
    output:
        path("${params.output_prefix}${prefix}_multiqc${params.assay_suffix}_data.zip"), emit: data
        path("${params.output_prefix}${prefix}_multiqc${params.assay_suffix}_report.html"), emit: html
    script:
        """
        multiqc -z -q -o . \\
                -n "${params.output_prefix}${prefix}_multiqc${params.assay_suffix}" . \\
                > /dev/null 2>&1
        

        # Renaming html file
        mv ${params.output_prefix}${prefix}_multiqc${params.assay_suffix}.html  \\
           ${params.output_prefix}${prefix}_multiqc${params.assay_suffix}_report.html
        """
  }




// This process runs cutadapt
process CUTADAPT {

    tag "Trimming off primers for ${sample_id} using cutadapt..."
    beforeScript "chmod +x ${baseDir}/bin/*"

    input:
        tuple val(sample_id), path(reads)
    output:
        tuple val(sample_id), path("${sample_id}${params.primer_trimmed_suffix}"), emit: reads
        tuple val(sample_id),  path("${sample_id}-cutadapt.log"), emit: logs
        tuple val(sample_id),  path("${sample_id}-trimmed-counts.tsv"), emit: trim_counts
    script:
        """
        cutadapt -g ${params.F_primer}  \\
                 -a ${params.R_primer} \\
                 -o ${sample_id}${params.primer_trimmed_suffix} \\
                 ${reads[0]} > ${sample_id}-cutadapt.log 2>&1

        paste <( printf "${sample_id}" ) \\
              <( grep "Total reads processed:" ${sample_id}-cutadapt.log | tr -s " " "\\t" | cut -f 4 | tr -d "," ) \\
              <( grep "Reads written (passing filters):" ${sample_id}-cutadapt.log | tr -s " " "\\t" | cut -f 5 | tr -d "," ) \\
              > ${sample_id}-trimmed-counts.tsv
        """
}




// This process combines the cutadapt logs and summarizes them. 
process COMBINE_CUTADAPT_LOGS_AND_SUMMARIZE {

    tag "Combining the logs generated by cutadapt..."

    input:
        path(counts)
        path(logs)
    output:
        path("${params.output_prefix}cutadapt${params.assay_suffix}.log"), emit: logs
        path("${params.output_prefix}trimmed-read-counts${params.assay_suffix}.tsv"), emit: counts
    script:
        """
        cat ${logs} > ${params.output_prefix}cutadapt${params.assay_suffix}.log
        
        cat <( printf "sample\\traw_reads\\tcutadapt_trimmed\\n" ) \\
            <( cat ${counts} ) > ${params.output_prefix}trimmed-read-counts${params.assay_suffix}.tsv
        """
}



//  This process runs quality filtering/trimming on input fastq files.
process BBDUK {


    tag "Quality filtering ${sample_id}s reads.."
    beforeScript "chmod +x ${baseDir}/bin/*"

    input:
        tuple val(sample_id), path(reads)
    output:
        tuple val(sample_id), path("${sample_id}${params.filtered_suffix}"), emit: reads
        tuple val(sample_id),  path("${sample_id}-bbduk.log"), emit: logs
        tuple val(sample_id),  path("${sample_id}-filtered-counts.tsv"), emit: filter_counts
    script:
      """
      bbduk.sh in=${reads[0]} out1=${sample_id}${params.filtered_suffix} \\
				  qtrim=r trimq=10 mlf=0.5 \\
                  minavgquality=${params.min_bbduk_avg_quality} \\
                  minlength=${params.min_bbduk_len} \\
                  > ${sample_id}-bbduk.log 2>&1

      paste <( printf "${sample_id}" ) <( grep "Input:"  ${sample_id}-bbduk.log | \\
      tr -s " " "\\t" | cut -f 2 ) <( grep "Result:"  ${sample_id}-bbduk.log | \\
      tr -s " " "\\t" | cut -f 2 ) > ${sample_id}-filtered-counts.tsv
      """
}



// This process combines the bbduk logs and summarizes them.
process COMBINE_BBDUK_LOGS_AND_SUMMARIZE {

    tag "Combining the logs generated by bbduk..."

    input:
        path(counts)
        path(logs)
    output:
        path("${params.output_prefix}bbduk${params.assay_suffix}.log"), emit: logs
        path("${params.output_prefix}filtered-read-counts${params.assay_suffix}.tsv"), emit: counts
    script:
        """
        cat ${logs} > ${params.output_prefix}bbduk${params.assay_suffix}.log

        cat <( printf "sample\\tinput_reads\\tfiltered_reads\\n" ) \\
            <( cat ${counts} ) > ${params.output_prefix}filtered-read-counts${params.assay_suffix}.tsv
        """
}






workflow quality_check {

    take:
    prefix_ch
    multiqc_config
    reads_ch
    

    main:
    fastqc_ch = FASTQC(reads_ch).flatten().collect()
    MULTIQC(prefix_ch, multiqc_config, fastqc_ch)
}

workflow {

    Channel.fromPath(params.csv_file)
               .splitCsv()
               .map{row -> tuple( "${row[0]}", [file("${row[1]}")] )}
               .set{reads_ch}   


    res_ch = quality_check(Channel.of(params.prefix), params.multiqc_config, reads_ch)
    CUTADAPT(reads_ch)
}
