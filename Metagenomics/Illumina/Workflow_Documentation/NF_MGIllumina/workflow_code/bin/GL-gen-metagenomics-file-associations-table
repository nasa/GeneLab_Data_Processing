#!/usr/bin/env python

"""
This is a program for generating the file-associations table needed by Curation for newly processed metagenomics datasets.
"""

import os
import sys
import argparse
import textwrap
import pandas as pd
import zipfile
import re

parser = argparse.ArgumentParser(description = "This program generates the file-assocations table needed by Curation for \
                                               newly processed metagenomics datasets. It is intended to be run after `GL-validate-processed-data` \
                                               has been run successfully.")
required = parser.add_argument_group('required arguments')
required.add_argument("-g", "--GLDS-ID", help = 'GLDS ID (e.g. "GLDS-276")', action = "store", required = True)
parser.add_argument("-i", "--isa-zip", help = "Appropriate ISA file for dataset (a zip archive, providing this will assume there is only one a_*amplicon* assay table in there, \
                                               if that's not the case, explicitly provide the assay table to the '-a' argument instead)",
                                       action = "store", default = "")
parser.add_argument("--assay-table", 
                    help = 'Appropriate assay table for dataset (this can be provided directly instead of being pulled from an ISA object)', 
                    action = "store", default = "")
parser.add_argument("--runsheet",
                    help = """
                           Input csv runsheet file used to run nextflow. This argument must be set when running the workflow with an OSD/GLDS accession as input as opposed to passing an input csv file.
                           This argument is used to get raw input file names that are used to retrieve raw read depths paer sample. 
                           """,
                    action = "store", default = "")

parser.add_argument("--output", 
                    help = 'Name of output log file (default: "<GLDS-ID>_[<output_prefix>]-associated-file-names.tsv", with appended prefix if one is provided)',
                    default = "", action = "store")
parser.add_argument("-p", "--output-prefix", help = "Output additional file prefix if there is one", action = "store", default = "")
parser.add_argument("--additional-string-to-remove-from-unique-filenames",
                    help = "If there is any additional text to remove from unqiue filenames, it can be provided here.",
                    action = "store")
parser.add_argument("--assay_suffix", help = "Genelab assay suffix", action = "store", default = "_GLmetagenomics")
parser.add_argument("--raw_file_prefix", help = "Prefix to be added to the raw data files alone (Default: <GLDS_ID>_metagenomics_)", action = "store", default ="")
parser.add_argument("--file_prefix", help = "Prefix to be added to all files except the raw files (Default: <GLDS_ID>_GLmetagenomics_)", action = "store", default ="")
parser.add_argument("--raw_suffix", help = "Raw reads suffix", action = "store", default ="_HRremoved_raw.fastq.gz")
parser.add_argument("--raw_R1_suffix", help = "Raw forward reads suffix", action = "store", default = "_R1_HRremoved_raw.fastq.gz")
parser.add_argument("--raw_R2_suffix", help = "Raw reverse reads suffix", action = "store", default = "_R2_HRremoved_raw.fastq.gz")
parser.add_argument("--filtered_suffix", help = "Filtered reads suffix", action = "store", default = "_filtered.fastq.gz")
parser.add_argument("--filtered_R1_suffix", help = "Filtered forward reads suffix", action = "store", default = "_R1_filtered.fastq.gz")
parser.add_argument("--filtered_R2_suffix", help = "Filtered reverse reads suffix", action = "store", default = "_R2_filtered.fastq.gz")
parser.add_argument("--processing_zip_file", help = "Specifies the name of processing_info.zip", 
                    action = "store", default = "processing_info.zip")
parser.add_argument("--readme", help = "Specifies the name of README.txt", 
                    action = "store", default = "README.txt")
parser.add_argument("--raw_reads_dir", help = "Specifies the name of the raw reads directory if they are to be included",
                    action = "store", default = "Raw_Sequence_Data/")
parser.add_argument("--fastqc_dir", help = "Specifies the location of fastqc and multiqc reports directory", 
                    action = "store", default = "FastQC_Outputs/")
parser.add_argument("--filtered_reads_dir", help = "Specifies the name of the filtered reads directory", 
                    action = "store", default = "Filtered_Sequence_Data/")
parser.add_argument("--read_based_dir", help = "Specifies the location of the directory containing results generated from read-based processing approach", 
                    action = "store", default = "Read-based_Processing/")
parser.add_argument("--assembly_based_dir", help = "Specifies the location of the directory containing results generated from assembly-based approach", 
                    action = "store", default = "Assembly-based_Processing/")
parser.add_argument("--assemblies_dir", help = "Specifies the location of the directory containing sample contig assemblies", 
                    action = "store", default = "Assembly-based_Processing/assemblies/")
parser.add_argument("--genes_dir", help = "Specifies the location of the directory containing predicted genes", 
                    action = "store", default = "Assembly-based_Processing/predicted-genes/")
parser.add_argument("--annotations_and_tax_dir", help = "Specifies the location of the directory containing contigs annotation and taxonomy", 
                    action = "store", default = "Assembly-based_Processing/annotations-and-taxonomy/")
parser.add_argument("--mapping_dir", help = "Specifies the location of the directory containing per-sample bam, coverage, and mapping info files", 
                    action = "store", default = "Assembly-based_Processing/read-mapping/")
parser.add_argument("--bins_dir", help = "Specifies the location of the directory containing recovered genome bins", 
                    action = "store", default = "Assembly-based_Processing/bins/")
parser.add_argument("--MAGs_dir", help = "Specifies the location of the directory containing meta-assembled genomes (MAGs)", 
                    action = "store", default = "Assembly-based_Processing/MAGs/")
parser.add_argument("--combined_output_dir", help = "Specifies the location of the directory containing contig annotation summary outputs with all samples combined", 
                    action = "store", default = "Assembly-based_Processing/combined-outputs/")
parser.add_argument("--single-ended", help = "Add this flag if data are single-end sequencing.", action = "store_true")
parser.add_argument("--R1-used-as-single-ended-data", help = "Provide this flag if processing only R1 reads as single-end (as the expected raw \
                    filename suffixes will have 'R1' in there)", 
                    action = "store_true")
parser.add_argument("--include-raw-multiqc-in-output",
                    help = "Provide this flag if wanting to include the raw multiqc zip in the file-associations output table (may be wanted for older datasets)", 
                    action = "store_true")
parser.add_argument("--use-sample-names-from-assay-table",
                    help = "Provide this flag if the unique filename strings in the processed outputs are taken directly from the \
                           'Sample Name' column of the input assay table.", action = "store_true")


if len(sys.argv)==1:
    parser.print_help(sys.stderr)
    sys.exit(0)

args = parser.parse_args()


# Setting some colors
tty_colors = {
    'green' : '\033[0;32m%s\033[0m',
    'yellow' : '\033[0;33m%s\033[0m',
    'red' : '\033[0;31m%s\033[0m'
}


######################### Aesthetic functions #########################
def color_text(text, color='green'):
    if sys.stdout.isatty():
        return tty_colors[color] % text
    else:
        return text


def wprint(text):
    """ Print wrapper """

    print(textwrap.fill(text, width=80, initial_indent="  ", 
          subsequent_indent="  ", break_on_hyphens=False))


def modify_symbolic_link(file_path):
    """ Modify symbolic link such that it retruns a string containing the parent dir and the base name"""
    full_path = os.path.realpath(file_path)
    parent_dir = os.path.basename(os.path.dirname(full_path))
    base_name = os.path.basename(full_path)
    mod_path = os.path.join(parent_dir, base_name)
    return(mod_path.replace("_", " ").rstrip("/"))


#################### End of Aesthetic functions #########################


def report_failure(message, color = "yellow"):
    print("")
    wprint(color_text(message, color))
    print("\nCuration file-associations table generation failed.\n")
    sys.exit(1)


def preflight_checks(isa_zip, assay_table):
    """Check that either one of isa_zip or assay_table is passed as argument"""
    # Ensure that at least one of isa_zip or assay_table is passed as argument
    if isa_zip == "" and assay_table == "":
        report_failure("This program requires either an input ISA object (passed to '-i') or a specific assay table (passed to '-a').")
    # Ensure that only one of isa_zip or assay_table is passed as argument 
    if isa_zip != "" and assay_table != "":
        report_failure("This program requires *only* an input ISA object (passed to '-i') or a specific assay table (passed to '-a'), not both.")


def check_for_file_and_contents(file_path):
    """Checks if file exists and that it is not empty"""
    if not os.path.exists(file_path):
        report_failure("The expected file '" + str(file_path) + "' does not exist.")
    if not os.path.getsize(file_path) > 0:
        report_failure("The file '" + str(file_path) + "' is empty.")


def get_assay_table_from_ISA(isa_zip):
    """ Tries to find a single assay table in an isa object """

    zip_file = zipfile.ZipFile(isa_zip)
    isa_files = zip_file.namelist()

    # Getting wanted filename (those that start with "a_"  and contains the word "metagenomic" seem to be what we want)
    wanted_file_list = [item for item in isa_files if item.startswith("a_") and item.find("metagenomic") != -1]
    if len(wanted_file_list) != 1:
        report_failure("We couldn't find the correct assay table in the ISA object, consider passing it directly to the '-a' argument.")

    wanted_file = wanted_file_list[0]

    df = pd.read_csv(zip_file.open(wanted_file), sep = "\t")

    return(df)


def get_assay_table(isa_zip, assay_table):
    """ Returns the assay table whether provided directly or pulled from ISA archive """

    # Get assay table if we are using an input isa object
    if isa_zip != "":
        # Check if ISA exists and thet it isn't empty
        check_for_file_and_contents(isa_zip)

        assay_table_df = get_assay_table_from_ISA(isa_zip)

    # Reading assay table if provided directly
    else:
        # Check if assay_table exists and that it isn't empty
        check_for_file_and_contents(assay_table)
        assay_table_df = pd.read_csv(assay_table, sep = "\t")

    return(assay_table_df)


def remove_suffixes(name, raw_file_prefix, raw_R1_suffix, raw_R2_suffix, raw_suffix):
    """ This removes expected prefixes and suffixes """

    # Removing expected prefix
    curr_name = name.replace(raw_file_prefix, "")

    # Removing potential suffixes (also checking R2 in case they are not 
    # in the appropriate order in the sample table, e.g. R2 before R1)
    curr_name = curr_name.replace(raw_R1_suffix, "")
    curr_name = curr_name.replace(raw_R2_suffix, "")
    curr_name = curr_name.replace(raw_suffix, "")

    return(curr_name)

def get_sample_names_and_unique_filenames(assay_table,  raw_file_prefix, raw_R1_suffix,
                                          raw_R2_suffix, raw_suffix,
                                         use_sample_names_from_assay_table,
                                         additional_string_to_remove_from_unique_filenames):
    """
    This gets the sample names ('Sample Name' column) from the assay table,
    and tries to get what would have been the unique filename prefixes generated from
    what's in the Raw Data File column of the assay table.

    Unless the --use-sample-names-from-assay-table flag was provided, then it just uses what's
    in the 'Sample Name' column.
    """

    sample_names = assay_table["Sample Name"].tolist()

    if use_sample_names_from_assay_table:
        unique_filename_prefixes = sample_names
        return(sample_names, unique_filename_prefixes)

    all_filenames = assay_table["Raw Data File"]

    unique_filename_prefixes = []
    
    # Attempting to split if they have multiple files (like paired-end)
    # and also removing the common prefixes and suffixes intending to create the same 
    # unique filenames used for processing

    for entry in all_filenames:

        # splitting if there are more than one (like with paired-end)
        curr_name = entry.split(",")[0]

        curr_name = remove_suffixes(curr_name, raw_file_prefix, raw_R1_suffix, raw_R2_suffix, raw_suffix)

        unique_filename_prefixes.append(curr_name)


    if additional_string_to_remove_from_unique_filenames:

        unique_filename_prefixes = [x.replace(additional_string_to_remove_from_unique_filenames, "") for x in unique_filename_prefixes]

    return(sample_names, unique_filename_prefixes)


def get_read_counts_from_raw_multiqc(raw_multiqc_stats_file_path,
                                      fastqc_dir, output_prefix,  raw_multiqc_zip):

    input_zip = os.path.join(fastqc_dir, output_prefix + raw_multiqc_zip)
    zip_file = zipfile.ZipFile(input_zip)
    df = pd.read_csv(zip_file.open(raw_multiqc_stats_file_path), sep = "\t", usecols = [0,6])
    df.columns = ["sample", "counts"]
    df.set_index("sample", inplace = True)

    return(df)


def get_read_count_from_df(sample_name, read_counts_tab, 
                           raw_suffix, raw_R1_suffix, single_ended, sample_raw_prefix_dict):

    if sample_raw_prefix_dict != "":
        return(round(read_counts_tab.at[sample_raw_prefix_dict[sample_name], "counts"]))

    if single_ended:
        return(round(read_counts_tab.at[str(sample_name) + \
                     raw_suffix.replace("_raw.fastq.gz", ""), "counts"]))
    else:
        return(round(read_counts_tab.at[str(sample_name) + \
                    raw_R1_suffix.replace("_raw.fastq.gz", ""), "counts"]))



def write_colnames(raw_reads_dir, filtered_reads_dir, fastqc_dir, 
                   assembly_based_dir, assemblies_dir, genes_dir, 
                   annotations_and_tax_dir, mapping_dir, bins_dir,
                   MAGs_dir, combined_output_dir, read_based_dir):
    """ Function to write the required column names"""

    # Get Parent directory and tag it unto the base directory name to confirm with what we expect as column headers
    # for sundirectory outputs
    assemblies_dir          = modify_symbolic_link(assemblies_dir) if os.path.islink(assemblies_dir) else assemblies_dir.replace("_", " ").rstrip("/")
    genes_dir               = modify_symbolic_link(genes_dir) if os.path.islink(genes_dir) else genes_dir.replace("_", " ").rstrip("/")
    annotations_and_tax_dir = modify_symbolic_link(annotations_and_tax_dir) if os.path.islink(annotations_and_tax_dir) else annotations_and_tax_dir.replace("_", " ").rstrip("/")
    mapping_dir             = modify_symbolic_link(mapping_dir) if os.path.islink(mapping_dir) else mapping_dir.replace("_", " ").rstrip("/")
    bins_dir                = modify_symbolic_link(bins_dir) if os.path.islink(bins_dir) else bins_dir.replace("_", " ").rstrip("/")
    MAGs_dir                = modify_symbolic_link(MAGs_dir) if os.path.islink(MAGs_dir) else MAGs_dir.replace("_", " ").rstrip("/")
    combined_output_dir     = modify_symbolic_link(combined_output_dir) if os.path.islink(combined_output_dir) else combined_output_dir.replace("_", " ").rstrip("/")


    colnames = ["Sample Name", 
                "Parameter Value[README]",
                f"Parameter Value[{raw_reads_dir}]",
                "Parameter Value[Read Depth]",
                "Unit",
                f"Parameter Value[{filtered_reads_dir}]",
                f"Parameter Value[{fastqc_dir}]",
                f"Parameter Value[{assembly_based_dir}]",
                f"Parameter Value[{assemblies_dir}]",
                f"Parameter Value[{genes_dir}]",
                f"Parameter Value[{annotations_and_tax_dir}]",
                f"Parameter Value[{mapping_dir}]",
                f"Parameter Value[{bins_dir}]",
                f"Parameter Value[{MAGs_dir}]",
                f"Parameter Value[{combined_output_dir}]",
                f"Parameter Value[{read_based_dir}]",
                "Parameter Value[Processing Info]"]
    return colnames



def create_constants(include_raw_multiqc_in_output, raw_multiqc_zip,
                     filtered_multiqc_zip, combined_prefix, assay_suffix):
    """A function to create lists of contants to be in creating a file association table"""
    if include_raw_multiqc_in_output:
        fastqc = [combined_prefix + raw_multiqc_zip, 
                  combined_prefix + filtered_multiqc_zip]
    else:
        fastqc = [combined_prefix + filtered_multiqc_zip]

    combined_outputs = [combined_prefix + f"Combined-gene-level-KO-function-coverages{assay_suffix}.tsv", 
                        combined_prefix + f"Combined-gene-level-KO-function-coverages-CPM{assay_suffix}.tsv",
                        combined_prefix + f"Combined-gene-level-taxonomy-coverages{assay_suffix}.tsv", 
                        combined_prefix + f"Combined-gene-level-taxonomy-coverages-CPM{assay_suffix}.tsv",
                        combined_prefix + f"Combined-contig-level-taxonomy-coverages{assay_suffix}.tsv", 
                        combined_prefix + f"Combined-contig-level-taxonomy-coverages-CPM{assay_suffix}.tsv"]

    read_based_outputs = [combined_prefix + f"Gene-families{assay_suffix}.tsv", 
                          combined_prefix + f"Gene-families-grouped-by-taxa{assay_suffix}.tsv", 
                          combined_prefix + f"Gene-families-cpm{assay_suffix}.tsv", 
                          combined_prefix + f"Gene-families-KO-cpm{assay_suffix}.tsv",
                          combined_prefix + f"Pathway-abundances{assay_suffix}.tsv", 
                          combined_prefix + f"Pathway-abundances-grouped-by-taxa{assay_suffix}.tsv", 
                          combined_prefix + f"Pathway-abundances-cpm{assay_suffix}.tsv", 
                          combined_prefix + f"Pathway-coverages{assay_suffix}.tsv",
                          combined_prefix + f"Pathway-coverages-grouped-by-taxa{assay_suffix}.tsv", 
                          combined_prefix + f"Metaphlan-taxonomy{assay_suffix}.tsv"]
    
    return fastqc, combined_outputs, read_based_outputs

def rusheet_to_dict(runsheet):
    """ Reads the input nextflow runsheet into a dataframe and converts it to 
        a dictionary with sample names as keys and raw reads forward prefix used
        by multiqc as values
    """
    def get_prefix(string):
        basename  = os.path.basename(string)
        index     = basename.rfind("_")
        return(basename[0:index])
    df                    = pd.read_csv(runsheet, usecols=["sample_id", "forward"])
    df['forward']         = df.forward.apply(lambda row : get_prefix(row))
    sample_to_prefix_dict = {k:v['forward'] for k,v in df.set_index("sample_id").T.to_dict().items()}
    return(sample_to_prefix_dict)

def create_association_table(header_colnames, assembly_overview_tab, fastqc, combined_outputs, read_based_outputs,
                             unique_filename_prefixes, read_count_tab, sample_file_dict, file_prefix,  combined_prefix,
                             readme, assay_suffix,  raw_file_prefix, raw_suffix, raw_R1_suffix, raw_R2_suffix,
                             filtered_suffix, filtered_R1_suffix, filtered_R2_suffix, processing_info,
                             single_ended, R1_used_as_single_ended_data, assemblies_dir, assembly_suffix, assembly_files,
                             genes_dir, mapping_dir, bins_overview, bins_dir_files, MAGs_overview, MAGs_dir_files,
                            MAG_KO_files_list, sample_raw_prefix_dict, read_count_unit = "read"):
    """Create association table and add data rows to it"""

    # Initialize association table
    association_df       = pd.DataFrame(columns = header_colnames)
    filtered_reads_count = combined_prefix + f"filtered-read-counts{assay_suffix}.tsv"
    # Create row
    for sample in unique_filename_prefixes:
        if sample_raw_prefix_dict != "":
            # This expects that sample input fastq files will always end with
            # HRremoved. if they don't, then you have to modify these hard coded strings.
            if (single_ended and R1_used_as_single_ended_data) or (not single_ended):
                raw_sample_name = re.sub("_R1_HRremoved$","", sample_raw_prefix_dict[sample])
            else:
                raw_sample_name = re.sub("_HRremoved$","", sample_raw_prefix_dict[sample])
        # Single-end (Paired-end data where only the forward reads were analyzed)
        if single_ended and R1_used_as_single_ended_data:
            # If only forward read was used, still want to include both foward and reverse read names 
            # in the "Raw Data" column because it is tied to the hosted raw data, not just what was used here
            curr_raw_data = [raw_file_prefix + sample + raw_R1_suffix,
                             raw_file_prefix + sample + raw_R2_suffix]

            if sample_raw_prefix_dict != "":
                curr_raw_data = [raw_sample_name + raw_R1_suffix, raw_sample_name  + raw_R2_suffix]

            curr_filt_data = [file_prefix + sample + filtered_R1_suffix, 
                              filtered_reads_count]
        # Single-end without reverse reads
        elif single_ended:
            curr_raw_data = [raw_file_prefix + sample + raw_suffix]

            if sample_raw_prefix_dict != "":
                curr_raw_data = [raw_sample_name + raw_suffix]


            curr_filt_data = [file_prefix + sample + filtered_suffix, 
                              filtered_reads_count]
        # Paired-end
        else:
            curr_raw_data = [raw_file_prefix + sample + raw_R1_suffix,
                             raw_file_prefix + sample + raw_R2_suffix]

            if sample_raw_prefix_dict != "":
                curr_raw_data = [raw_sample_name + raw_R1_suffix, raw_sample_name  + raw_R2_suffix]
            
            curr_filt_data = [file_prefix + sample + filtered_R1_suffix, 
                              file_prefix + sample + filtered_R2_suffix, 
                              filtered_reads_count]
        # Get sample raw read count
        curr_read_count = get_read_count_from_df(sample, read_count_tab, raw_suffix,
                                                 raw_R1_suffix, single_ended, sample_raw_prefix_dict)
        
        read_count_tab['samples'] = read_count_tab.index
        contains_sample           = read_count_tab['samples'].str.contains
        # Only adding file to list if it exists and isn't empty (easier for curation this way)
        curr_path = os.path.join(assemblies_dir, sample + assembly_suffix)

        if os.path.exists(curr_path) and os.path.getsize(curr_path) > 0:
            curr_assembly = [file_prefix + sample + assembly_suffix] + assembly_files
        else:
            curr_assembly = [""]

        # Only adding file to list if it exists and isn't empty (easier for curation this way)
        curr_genes = []
        gene_suffixes = ["-genes.faa", "-genes.fasta", "-genes.gff"] 
        for ext in gene_suffixes:
            curr_path = os.path.join(genes_dir, sample + ext)
            
            if os.path.exists(curr_path) and os.path.getsize(curr_path) > 0:
                curr_genes.append(file_prefix + sample + ext)

        # Adding empty value if all 3 missing (which i don't think happens as the gff has content either way)
        if len(curr_genes) == 0:
            curr_genes = [""]

        # These have headers even if no data for a sample, so no complications about being empty
        curr_annots = [file_prefix + sample + "-gene-coverage-annotation-and-tax.tsv", 
                       file_prefix + sample + "-contig-coverage-and-tax.tsv"]

        # Only adding file to list if it exists and isn't empty (easier for curation this way)
        curr_read_mapping = []
        mapping_suffixes = [".bam", "-mapping-info.txt", "-metabat-assembly-depth.tsv"]
        for ext in mapping_suffixes:
            curr_path = os.path.join(mapping_dir, sample + ext)

            if os.path.exists(curr_path) and os.path.getsize(curr_path) > 0:
                curr_read_mapping.append(file_prefix + sample + ext)

        # Adding empty value if all 3 missing
        if len(curr_read_mapping) == 0:
            curr_read_mapping = [""]

        if bins_overview[0] == f"{combined_prefix}bins-overview{assay_suffix}.tsv":
            curr_bins = bins_overview + [file_prefix + file for file in bins_dir_files if file.startswith(sample)]
        else:
            curr_bins = [""]

        if MAGs_overview[0] == f"{combined_prefix}MAGs-overview{assay_suffix}.tsv":
            curr_MAGs = MAGs_overview + [file_prefix + file for file in MAGs_dir_files if file.startswith(sample)] + MAG_KO_files_list
        else:
            curr_MAGs = [""]

        curr_row_as_list = [sample_file_dict[sample],
                            readme,
                            ", ".join(curr_raw_data),
                            curr_read_count, 
                            read_count_unit,
                            ", ".join(curr_filt_data),
                            ", ".join(fastqc),
                            assembly_overview_tab,
                            ", ".join(curr_assembly),
                            ", ".join(curr_genes),
                            ", ".join(curr_annots),
                            ", ".join(curr_read_mapping),
                            ", ".join(curr_bins),
                            ", ".join(curr_MAGs),
                            ", ".join(combined_outputs),
                            ", ".join(read_based_outputs),
                            processing_info]

        # Append row to the association dataframe
        association_df.loc[len(association_df)] = curr_row_as_list

    return association_df



def write_association_table(outfile, association_df):
    """Write to csv file"""
    # Writing out
    association_df.to_csv(outfile, sep = "\t", index = False)


def main():

    ### Set variables  ###
    # Directories
    fastqc_dir = str(args.fastqc_dir)
    raw_reads_dir = str(args.raw_reads_dir)
    filtered_reads_dir = str(args.filtered_reads_dir)
    assembly_based_dir = str(args.assembly_based_dir)
    assemblies_dir = str(args.assemblies_dir)
    genes_dir = str(args.genes_dir)
    annotations_and_tax_dir = str(args.annotations_and_tax_dir)
    mapping_dir = str(args.mapping_dir)
    bins_dir = str(args.bins_dir)
    MAGs_dir = str(args.MAGs_dir)
    combined_output_dir = str(args.combined_output_dir)
    read_based_dir = str(args.read_based_dir)
    
    raw_reads_dir = raw_reads_dir.replace("_", " ").rstrip("/")
    filtered_reads_dir = filtered_reads_dir.replace("_", " ").rstrip("/")
    assembly_based_dir = assembly_based_dir.replace("_", " ").rstrip("/")
    annotations_and_tax_dir = annotations_and_tax_dir.replace("_", " ").rstrip("/")
    combined_output_dir = combined_output_dir.replace("_", " ").rstrip("/")
    read_based_dir = read_based_dir.replace("_", " ").rstrip("/")
    fastqc_dir     = fastqc_dir.replace("_", " ").rstrip("/")


    # Suffixes
    filtered_suffix = str(args.filtered_suffix)
    filtered_R1_suffix = str(args.filtered_R1_suffix)
    filtered_R2_suffix = str(args.filtered_R2_suffix)
    raw_suffix = str(args.raw_suffix)
    raw_R1_suffix = str(args.raw_R1_suffix)
    raw_R2_suffix = str(args.raw_R2_suffix)
    if args.R1_used_as_single_ended_data:
        raw_suffix = raw_R1_suffix
        # Just in case user only specified --R1-used-as-single-ended, but didn't specify --single-ended
        args.single_ended = True 
    assay_suffix = str(args.assay_suffix) 
    assembly_suffix = "-assembly.fasta"

    # This one is only used for the raw data files
    raw_file_prefix = f"{args.GLDS_ID}_metagenomics_" if args.raw_file_prefix == "" else  str(args.raw_file_prefix)
    file_prefix = f"{args.GLDS_ID}_GMetagenomics_" if args.file_prefix == "" else  str(args.file_prefix)
    raw_multiqc_zip = f"raw_multiqc{assay_suffix}_report.zip"
    filtered_multiqc_zip = f"filtered_multiqc{assay_suffix}_report.zip"
    output_prefix = str(args.output_prefix)
    combined_prefix = file_prefix + output_prefix
    raw_multiqc_stats_file_path = output_prefix + f"raw_multiqc_report.zip".split(".")[0] + \
                                  f"/{output_prefix}raw_multiqc_data/multiqc_general_stats.txt"
    processing_info = combined_prefix + str(args.processing_zip_file)

    assembly_overview_tab = f"{combined_prefix}Assembly-based-processing-overview{assay_suffix}.tsv"
    failed_assemblies = os.path.join(args.assemblies_dir, f"{output_prefix}Failed-assemblies{assay_suffix}.tsv")
    bins_overview_tab = os.path.join(args.bins_dir, f"{output_prefix}bins-overview{assay_suffix}.tsv")
    mags_overview_tab = os.path.join(args.MAGs_dir,f"{output_prefix}MAGs-overview{assay_suffix}.tsv")
    mag_ko_annotation = os.path.join(args.MAGs_dir, f"{output_prefix}MAG-level-KO-annotations{assay_suffix}.tsv")
    mag_kegg_tab      = os.path.join(args.MAGs_dir, f"{output_prefix}MAG-KEGG-Decoder-out{assay_suffix}.tsv")
    mag_kegg_html     = os.path.join(args.MAGs_dir, f"{output_prefix}MAG-KEGG-Decoder-out{assay_suffix}.html")


    # Set output file name
    if args.output == "" and output_prefix != "":
        outfile = f"{args.GLDS_ID}_{output_prefix}-associated-file-names.tsv"
    elif args.output == "":
        outfile = f"{args.GLDS_ID}-associated-file-names.tsv"
    else:
        outfile = args.output
    
    readme = combined_prefix + args.readme
    include_raw_multiqc_in_output = str(args.include_raw_multiqc_in_output)


    if os.path.exists(failed_assemblies):
        assembly_files = [f"{combined_prefix}assembly-summaries{assay_suffix}.tsv",
                          f"{combined_prefix}Failed-assemblies{assay_suffix}.tsv"]
    else:
        assembly_files = [f"{combined_prefix}assembly-summaries{assay_suffix}.tsv"]

    if os.path.exists(bins_overview_tab):
        bins_overview = [f"{combined_prefix}bins-overview{assay_suffix}.tsv"]
        bins_dir_files = [file for file in os.listdir(args.bins_dir) if file.endswith(".fasta")]
    else:
        bins_overview = [""]
        bins_dir_files = [""]

    if os.path.exists(mags_overview_tab):
        MAGs_overview = [f"{combined_prefix}MAGs-overview{assay_suffix}.tsv"]
        MAGs_dir_files = [file for file in os.listdir(args.MAGs_dir) if file.endswith(".fasta")]
    else:
        MAGs_overview = [""]
        MAGs_dir_files = [""]

    MAG_KO_files_list = []
    if os.path.exists(mag_ko_annotation):
        MAG_KO_files_list.append(f"{combined_prefix}MAG-level-KO-annotations{assay_suffix}.tsv")
    if os.path.exists(mag_kegg_tab):
        MAG_KO_files_list.append(f"{combined_prefix}MAG-KEGG-Decoder-out{assay_suffix}.tsv")
    if os.path.exists(mag_kegg_html):
        MAG_KO_files_list.append(f"{combined_prefix}MAG-KEGG-Decoder-out{assay_suffix}.html")


    # Check that either of ISA zip or assay table is passed as argument
    preflight_checks(args.isa_zip, args.assay_table)

    assay_table = get_assay_table(args.isa_zip, args.assay_table)

    sample_names, unique_filename_prefixes = get_sample_names_and_unique_filenames(assay_table,  raw_file_prefix, raw_R1_suffix,
                                                                                   raw_R2_suffix, raw_suffix, 
                                                                                   args.use_sample_names_from_assay_table,
                                                                                   args.additional_string_to_remove_from_unique_filenames)

    sample_file_dict = dict(zip(unique_filename_prefixes, sample_names))

    read_counts_df = get_read_counts_from_raw_multiqc(raw_multiqc_stats_file_path, args.fastqc_dir, 
                                                      output_prefix,  raw_multiqc_zip)
    
    ###################################  Write file association table ##########################################
    header = write_colnames(raw_reads_dir, filtered_reads_dir, fastqc_dir, 
                            assembly_based_dir, args.assemblies_dir, args.genes_dir, 
                            args.annotations_and_tax_dir, args.mapping_dir, args.bins_dir,
                            args.MAGs_dir, args.combined_output_dir, read_based_dir)
    
    fastqc, combined_outputs, read_based_outputs = create_constants(include_raw_multiqc_in_output, raw_multiqc_zip,
                                             filtered_multiqc_zip, combined_prefix, assay_suffix)
   

    # Retrieve a dictionary with sample names as keys and raw fatqfile prefix as values 
    sample_raw_prefix_dict = rusheet_to_dict(args.runsheet) if args.runsheet != "" else ""

    association_df = create_association_table(header, assembly_overview_tab, fastqc, combined_outputs, read_based_outputs,
                                              unique_filename_prefixes, read_counts_df, sample_file_dict, file_prefix,
                                              combined_prefix,readme, assay_suffix,  raw_file_prefix, raw_suffix, 
                                              raw_R1_suffix, raw_R2_suffix, filtered_suffix, filtered_R1_suffix, 
                                              filtered_R2_suffix, processing_info, args.single_ended,
                                              args.R1_used_as_single_ended_data, args.assemblies_dir, assembly_suffix, assembly_files,
                                              args.genes_dir, args.mapping_dir, bins_overview, bins_dir_files, MAGs_overview,
                                              MAGs_dir_files, MAG_KO_files_list, sample_raw_prefix_dict, read_count_unit = "read")
    

    write_association_table(outfile, association_df)


if __name__ == "__main__":
    main()
